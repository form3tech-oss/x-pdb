{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>x-pdb scales Pod Disruption Budgets beyond a single Kubernetes cluster.</p> <p>It protects your workloads from getting disrupted by maintenance activity such as deploying configuration changes or draining nodes.</p> <p>x-pdb runs as a deployment and is deployed to all clusters. It rejects Pod deletions and evictions when not enough Pods are available. x-pdb provides a <code>XPodDisruptionBudget</code> resources which allows you to define your Pod Disruption Budget across multiple clusters.</p>"},{"location":"#how-x-pdb-works","title":"How x-pdb works","text":"<p>x-pdb hooks into the <code>DELETE &lt;namespace&gt;/pods/&lt;pod&gt;</code> and <code>POST &lt;namespace&gt;/pods/&lt;pod&gt;/eviction</code> API calls using a <code>ValidatingWebhookConfiguration</code>. When called, x-pdb acquires a lock on all clusters to prevent race conditions if evictions or deletions happen at the same moment.</p> <p>Once the lock is acquired, x-pdb reads the pod status on all clusters: the number of expected pods and number of healthy pods. It then computes if a eviction/deletion is allowed. It is the same behavior as Kubernetes' pod disruption budget.</p>"},{"location":"#locking-mechanism","title":"Locking mechanism","text":"<p>X-PDB acquires a lock on the remote clusters using a grpc API. The lock is valid for a specific <code>namespace/selector</code> combination and it has a <code>leaseHolderIdentity</code>. This is the owner of the given lock.</p> <p>The lock is valid for 5 seconds. After that it can be re-acquired or taken over by a different holder. The lock prevents a race condition which can occur if multiple evictions happen simultaneously across clusters which would lead to inconsistent data and wrong decisions. E.g. a read can happen while a eviction is being processed in a different cluster which would lead to multiple evictions happen at the same time - this could break the pod disruption budget.</p> <p>We leave the lock as it is and DO NOT unlock it after the admission webhook has finished processing. Once the lock expires it can be re-acquired or taken over. We rely on the caller to retry the eviction or deletion of a Pod.</p> <p>Why 5 seconds? - Why leave it locked when we've finished processing?</p> <p>We need to lock evictions for a period of time after we have returned the admission result to allow kube-apiserver to update the Pod (set the <code>deletionTimestamp</code>).</p> <p>The lease duration should be higher than the sum of the following duration:</p> <ul> <li>the round-trip latency across all clusters (1-2 seconds)</li> <li>the processing time of the x-pdb http server (&lt;1 second)</li> <li>the time kube-apiserver needs to process the x-pdb admission control response   and the time it takes until the desired action (evict/delete pod) is observable through the kube-apiserver (1-2 seconds)</li> <li>a generous surcharge (1-... seconds)</li> </ul>"},{"location":"configuring-disruption-probes/","title":"Configuring Disruption Probes","text":"<p>x-pdb allows workload owners to define a disruption probe endpoint.</p> <p>This endpoint is used by x-pdb to make a decision whether or not a disruption is allowed. Without the probe endpoint, x-pdb considers only pod readiness as a indicator of pod healthiness.</p> <p>With the probe endpoint configured, x-pdb will ask the probe endpoint whether or not the disruption is allowed.</p> <p>The probe endpoint is just a gRPC server, see details below.</p> <p>It might be helpful to probe internal state of some workloads like databases to verify wether an eviction can happen or not. Database Raft groups might become unavailable if a given pod is disrupted. In these cases workload owners might want to block disruptions to happen, even if all pods are ready.</p> <p>Example use-cases:</p> <ul> <li>assess health of a database cluster as a whole before allowing the deletion of a single pod, as this disruption may add more pressure on the database.</li> <li>assess the state of raft group leaders in a cluster before allowing an eviction</li> <li>assess the replication lag of database clusters</li> <li>assess if any long-running queries/jobs or backup are running, where a deletion can cause a problem</li> </ul> <pre><code>apiVersion: x-pdb.form3.tech/v1alpha1\nkind: XPodDisruptionBudget\nmetadata:\n  name: kube-dns\n  namespace: kube-system\nspec:\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  probe:\n    endpoint: opensearch-disruption-probe.opensearch.svc.cluster.local:8080\n</code></pre>"},{"location":"configuring-disruption-probes/#tls-and-authentication","title":"TLS and authentication","text":"<p>At this point, the communication between x-pdb and the probe server does not use mutual TLS and only validates the server certificate presented by the probe endpoint and verifies it has been issued by the CA defined in <code>--controller-certs-dir</code>.</p>"},{"location":"configuring-disruption-probes/#disruptionprobe-server","title":"DisruptionProbe Server","text":"<p>The DisruptionProbe server allows the client to ask if a disruption for a given pod and XPDB resource is allowed.</p> <pre><code>service DisruptionProbe {\n  // Sends a IsDisruptionAllowed request\n  rpc IsDisruptionAllowed(IsDisruptionAllowedRequest) returns (IsDisruptionAllowedResponse) {}\n}\n\n// The request message containing the user's name.\nmessage IsDisruptionAllowedRequest {\n  string pod_name = 1;\n  string pod_namespace = 2;\n  string xpdb_name = 3;\n  string xpdb_namespace = 4;\n}\n\n// The response message containing the greetings\nmessage IsDisruptionAllowedResponse {\n  bool is_allowed = 1;\n  string error = 2;\n}\n</code></pre>"},{"location":"configuring-xpdb/","title":"Configuring XPodDisruption Resource","text":""},{"location":"configuring-xpdb/#nomenclature","title":"Nomenclature","text":"term description local cluster This is the Kubernetes cluster where a pod deletion or eviction is being requested. remote clusters These are the clusters which are not handling the pod eviction / deletion."},{"location":"configuring-xpdb/#configuration-and-behavior","title":"Configuration and Behavior","text":"<p>X-PDB works under the assumption that your workloads are structured in a similar way across clusters, i.e. that pods that you want to protect sit in the same namespace no matter which cluster you look at.</p> <p>The <code>XPodDisruptionBudget</code> resources looks and feels just like <code>PodDisruptionBudget</code> resources:</p> <ul> <li> <p>A label selector <code>.spec.selector</code> to specify the set of pods to which it applies. The <code>selector</code> applies to all clusters for decision making, not just the local one where pod eviction/deletion is happening.</p> </li> <li> <p><code>.spec.minAvailable</code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage.</p> </li> <li><code>.spec.maxUnavailable</code> which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.</li> </ul> <p>You can specify only one of <code>maxUnavailable</code> and <code>minAvailable</code> in a single PodDisruptionBudget. <code>maxUnavailable</code> can only be used to control the eviction of pods that have an associated controller managing them.</p> <p>In addition to that, <code>XPodDisruptionBudget</code> has the following fields:</p> <ul> <li><code>.spec.suspend</code> which allows you to disable the XPDB resource. This allows all pod deletions/evictions. It is intended to be used as a break-glass procedure to allow engineers to take manual action. The suspension is configured on a per-cluster basis and affects only local pods. I.e. other clusters that run x-pdb will not be able to evict pods if there isn't enough disruption budget available globally.</li> <li><code>.spec.probe</code> that allows workload owners to define a disruption probe endpoint. Without a probe, x-pdb will only consider pod readiness as an indicator of healthiness and compute the disruption verdict based on that. With <code>.spec.probe</code>, x-pdb considers the response of the probe endpoint as well.</li> </ul> <p>It is irrelevant for <code>x-pdb</code> if the remote cluster has a <code>XPodDisruptionBudget</code> resource and whether or not the configuration match.</p> <p>The user is supposed to deploy the <code>XPodDisruptionBudget</code> to all clusters. It may lead to unexpected disruptions when the resource is missing.</p> <pre><code>apiVersion: x-pdb.form3.tech/v1alpha1\nkind: XPodDisruptionBudget\nmetadata:\n  name: kube-dns\n  namespace: kube-system\nspec:\n  # Specify either `minAvailable` or `maxUnavailable`\n  # Both percentages and numbers are supported\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n</code></pre>"},{"location":"configuring-xpdb/#grpc-state-server","title":"gRPC State Server","text":"<p>In order for x-pdb servers to communicate between each other they expose a gRPC state server interface with the following APIs. It allows x-pdb to asses the health of pods on remote clusters.</p> <p>The communication between x-pdb servers is secured using mutual TLS. The certificate directory can be configured with <code>--controller-certs-dir</code> which is supposed to contain <code>ca.crt</code>, <code>tls.crt</code> and <code>tls.key</code> files.</p> <pre><code>// State is the service that allows x-pdb servers to talk with\n// each other.\nservice State {\n  // Acquires a lock on the local cluster using the specified leaseHolderIdentity.\n  rpc Lock(LockRequest) returns (LockResponse) {}\n\n  // Frees a lock on the local cluster if the lease identity matches.\n  rpc Unlock(UnlockRequest) returns (UnlockResponse) {}\n\n  // Calculates the expected count based off the Deployment/StatefulSet/ReplicaSet number of replicas or - if implemented - a `scale` sub resource.\n  rpc GetState(GetStateRequest) returns (GetStateResponse) {}\n}\n</code></pre>"},{"location":"failure-scenarios/","title":"Failure Scenarios","text":""},{"location":"failure-scenarios/#1-a-remote-cluster-is-unavailable","title":"1. A remote Cluster is unavailable","text":"<p>Scenario: one of the remote clusters is unavailable, e.g. due to a network issue or that all x-pdb pods on the remote cluster are unavailable.</p>"},{"location":"failure-scenarios/#impact-pod-evictionsdeletions-are-blocked","title":"Impact: pod evictions/deletions are blocked","text":"<p>Because the state of the remote clusters is undefined, x-pdb will reject all eviction/deletions. Though this applies only for pods which have a <code>XPodDisruptionBudget</code> configured.</p> <p>\u26a0\ufe0f Warning: This includes blocking node pressure evictions.</p>"},{"location":"failure-scenarios/#mitigation","title":"Mitigation","text":"<p>If a cluster is taken down for maintenance for a extended period of time you should consider removing that cluster from the <code>remoteEndpoint</code> configuration, so that x-pdb ignores that cluster entirely.</p> <p>In an incident scenario you can consider to set the <code>failurePolicy=Ignore</code> to NOT block deletions/evictions. Only do it if you can accept the risk. Your workloads are no longer protected.</p> <p>Further, you can temporarily delete the <code>XPodDisruptionBudget</code> resource or setting <code>spec.suspend=true</code> which effectively make x-pdb ignore it.</p>"},{"location":"failure-scenarios/#2-x-pdb-pods-are-unavailable-within-the-cluster","title":"2. x-pdb pods are unavailable within the cluster","text":"<p>Scenario: x-pdb is unavailable within the cluster, from the perspective of kube-apiserver. That could be due to network policies, TLS misconfiguration, service selector misconfiguration, pods being not ready or other means. It is likely, that x-pdb is also unavailable from the perspective of other clusters (see scenario above).</p>"},{"location":"failure-scenarios/#impact-all-pod-evictionsdeletions-are-blocked","title":"Impact: all pod evictions/deletions are blocked","text":"<p>Depending on the <code>failurePolicy</code> on the webhook, kube-apiserver will reject all pod evictions / deletions due to the unavailability of the webhook.</p> <p>\u26a0\ufe0f Warning: This includes blocking node pressure evictions.</p>"},{"location":"failure-scenarios/#mitigation_1","title":"Mitigation","text":"<p>In an incident scenario you can consider to set the <code>failurePolicy=Ignore</code> to NOT block deletions/evictions. Only do it if you can accept the risk. Your workloads are no longer protected.</p>"},{"location":"failure-scenarios/#reliability-recommendations","title":"Reliability recommendations","text":""},{"location":"failure-scenarios/#1-enable-x-pdb-only-for-specific-workloads","title":"1. Enable x-pdb only for specific workloads","text":"<p>To limit the impact on a remote cluster being unavailable, configure the x-pdb webhook to only apply to certain namespaces.</p> <pre><code>webhook:\n  namespaceSelector: {}\n    matchExpressions:\n      - key: x-pdb.form3.tech/enabled\n        operator: Exists\n</code></pre>"},{"location":"failure-scenarios/#2-use-suspend","title":"2. use <code>suspend</code>","text":"<p>Use XPodDisruptionBudget <code>.spec.suspend</code> to temporarily disable the XPDB in case of an incident. Be aware that your workloads are no longer protected.</p>"},{"location":"failure-scenarios/#3-use-disruption-probes","title":"3. Use disruption probes","text":"<p>Use disruption probes to assess if a workload is stable enough to deal with a disruption.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>x-pdb provides a helm chart for as method of installation. It is recommended to deploy <code>x-pdb</code> into a dedicated namespace and not co-locate it with other applications in existing namespaces such as <code>kube-system</code>.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>x-pdb is supposed to be deployed into multiple clusters. Each x-pdb deployment needs to be accessible from other clusters. Implementation depends on your environment, most likely you'll have a Service with type=LoadBalancer to expose x-pdb to your other clusters.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Helm</li> <li>Have multiple Kubernetes clusters with a supported version</li> <li>Have cert-manager installed or hand cranked TLS certificates ready to be deployed</li> </ul>"},{"location":"getting-started/#1-installing-x-pdb","title":"1. Installing x-pdb","text":"<pre><code>helm repo add x-pdb https://form3tech-oss.github.io/x-pdb/\nhelm repo update\nhelm install x-pdb/x-pdb -n x-pdb \\\n    --create-namespace \\\n    --set controller.clusterID cluster-1\n</code></pre>"},{"location":"getting-started/#2-configuring-x-pdb","title":"2. Configuring x-pdb","text":"<p>A full list of available Helm values is in the x-pdb GitHub repository.</p> <p>The example below shows the most important configuration values which you'll need to set in order to get x-pdb up and running.</p> <pre><code>controller:\n  # set cluster id of _this_ deployment\n  clusterID: \"blue\"\n  # remote endpoint which x-pdb will ask for approving an eviction\n  remoteEndpoints:\n    - x-pdb.lb.grey.cluster.local\n    - x-pdb.lb.gold.cluster.local\n  tls:\n    # let cert-manager mint a certificate for x-pdb controller\n    certManager:\n      enabled: true\n      issuerRef:\n        group: cert-manager.io\n        kind: ClusterIssuer\n        name: my-example-issuer\n      dnsNames:\n        - x-pdb.lb.blue.cluster.local\nservice:\n  controller:\n    type: LoadBalancer\n\nwebhook:\n  # intercept evictions only in certain namespaces\n  namespaceSelector:\n    matchLabels:\n      \"x-pdb-enabled\": \"true\"\n  # let cert-manager mint a certificate for x-pdb webhook\n  tls:\n    certManager:\n      enabled: true\n      issuerRef:\n        group: cert-manager.io\n        kind: ClusterIssuer\n        name: my-example-issuer\npriorityClassName: system-cluster-critical\nserviceMonitor:\n  enabled: true\n</code></pre>"},{"location":"metrics-slos/","title":"Metrics &amp; SLOs","text":"<p>x-pdb exposes Prometheus metrics on the <code>/metrics</code> path. To enable it, set the <code>serviceMonitor.enabled</code> Helm flag to true.</p>"},{"location":"metrics-slos/#metrics","title":"Metrics","text":""},{"location":"metrics-slos/#x-pdb","title":"x-pdb","text":"Name Type Description <code>pod_eviction_rejected</code> Counter Represents the number of eviction which have been rejected through x-pdb. <code>pod_matches_multiple_xpdbs</code> Counter A eviction attempt for a pod has been observed which matches multiple XPDBs. This is a invalid configuration and must be fixed. <code>lock_errors</code> Counter Counter that represents the number of errors when obtaining locks for xpdb."},{"location":"metrics-slos/#grpc-metrics","title":"grpc metrics","text":"<p>x-pdb exposes GRPC metrics for both client and server which allow you to get insights into latency and availability of the remote x-pdb servers.</p>"},{"location":"metrics-slos/#slos","title":"SLOs","text":""},{"location":"metrics-slos/#availability","title":"Availability","text":"<p>There should be at least one pod ready to serve traffic at any time, preferably measured from both <code>kube-apiserver</code> and <code>x-pdb</code> on other clusters.</p> <pre><code>sum(increase(apiserver_admission_webhook_fail_open_count{name=~\".*x-pdb.*\"}[5m]))\n</code></pre>"},{"location":"metrics-slos/#latency","title":"Latency","text":"<p>The amount of time x-pdb needs to respond to a admission webhook, preferably measured from the kube-apiserver. It should take less than 150ms for <code>x-pdb</code> to respond to admission requests on the p99. The threshold may vary in your environment, depending on the cross-cluster latency.</p> <pre><code>histogram_quantile(0.99,\n    sum(rate(apiserver_admission_webhook_admission_duration_seconds_bucket{name=~\".*x-pdb.*\"}[5m])) by (le, name)\n)\n</code></pre>"}]}