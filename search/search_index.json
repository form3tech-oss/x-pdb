{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>x-pdb scales Pod Disruption Budgets beyond a single Kubernetes cluster.</p> <p>It protects your workloads from getting disrupted by maintenance activity such as deploying configuration changes or draining nodes.</p> <p>x-pdb runs as a deployment and is deployed to all clusters. It rejects Pod deletions and evictions when not enough Pods are available. x-pdb provides a <code>XPodDisruptionBudget</code> resources which allows you to define your Pod Disruption Budget across multiple clusters.</p>"},{"location":"#how-x-pdb-works","title":"How x-pdb works","text":"<p>x-pdb hooks into the <code>DELETE &lt;namespace&gt;/pods/&lt;pod&gt;</code> and <code>POST &lt;namespace&gt;/pods/&lt;pod&gt;/eviction</code> API calls using a <code>ValidatingWebhookConfiguration</code>. When called, x-pdb acquires a lock on all clusters to prevent race conditions if evictions or deletions happen at the same moment.</p> <p>Once the lock is acquired, x-pdb reads the pod status on all clusters: the number of expected pods and number of healthy pods. It then computes if a eviction/deletion is allowed. It is the same behavior as Kubernetes' pod disruption budget.</p>"},{"location":"#locking-mechanism","title":"Locking mechanism","text":"<p>X-PDB acquires a lock on the remote clusters using a grpc API. The lock is valid for a specific <code>namespace/selector</code> combination and it has a <code>leaseHolderIdentity</code>. This is the owner of the given lock.</p> <p>The lock is valid for 5 seconds. After that it can be re-acquired or taken over by a different holder. The lock prevents a race condition which can occur if multiple evictions happen simultaneously across clusters which would lead to inconsistent data and wrong decisions. E.g. a read can happen while a eviction is being processed in a different cluster which would lead to multiple evictions happen at the same time - this could break the pod disruption budget.</p> <p>We leave the lock as it is and DO NOT unlock it after the admission webhook has finished processing. Once the lock expires it can be re-acquired or taken over. We rely on the caller to retry the eviction or deletion of a Pod.</p> <p>Why 5 seconds? - Why leave it locked when we've finished processing?</p> <p>We need to lock evictions for a period of time after we have returned the admission result to allow kube-apiserver to update the Pod (set the <code>deletionTimestamp</code>).</p> <p>The lease duration should be higher than the sum of the following duration:</p> <ul> <li>the round-trip latency across all clusters (1-2 seconds)</li> <li>the processing time of the x-pdb http server (&lt;1 second)</li> <li>the time kube-apiserver needs to process the x-pdb admission control response   and the time it takes until the desired action (evict/delete pod) is observable through the kube-apiserver (1-2 seconds)</li> <li>a generous surcharge (1-... seconds)</li> </ul>"},{"location":"code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [INSERT CONTACT METHOD]. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"configuring-disruption-probes/","title":"Configuring Disruption Probes","text":"<p>x-pdb allows workload owners to define a disruption probe endpoint.</p> <p>This endpoint is used by x-pdb to make a decision whether or not a disruption is allowed. Without the probe endpoint, x-pdb considers only pod readiness as a indicator of pod healthiness.</p> <p>With the probe endpoint configured, x-pdb will ask the probe endpoint whether or not the disruption is allowed.</p> <p>The probe endpoint is just a gRPC server, see details below.</p> <p>It might be helpful to probe internal state of some workloads like databases to verify wether an eviction can happen or not. Database Raft groups might become unavailable if a given pod is disrupted. In these cases workload owners might want to block disruptions to happen, even if all pods are ready.</p> <p>Example use-cases:</p> <ul> <li>assess health of a database cluster as a whole before allowing the deletion of a single pod, as this disruption may add more pressure on the database.</li> <li>assess the state of raft group leaders in a cluster before allowing an eviction</li> <li>assess the replication lag of database clusters</li> <li>assess if any long-running queries/jobs or backup are running, where a deletion can cause a problem</li> </ul> <pre><code>apiVersion: x-pdb.form3.tech/v1alpha1\nkind: XPodDisruptionBudget\nmetadata:\n  name: opensearch\n  namespace: opensearch\nspec:\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      k8s-app: opensearch\n  probe:\n    endpoint: opensearch-disruption-probe.opensearch.svc.cluster.local:8080\n</code></pre>"},{"location":"configuring-disruption-probes/#tls-and-authentication","title":"TLS and authentication","text":"<p>At this point, the communication between x-pdb and the probe server does not use mutual TLS and only validates the server certificate presented by the probe endpoint and verifies it has been issued by the CA defined in <code>--controller-certs-dir</code>.</p>"},{"location":"configuring-disruption-probes/#disruptionprobe-server","title":"DisruptionProbe Server","text":"<p>The DisruptionProbe server allows the client to ask if a disruption for a given pod and XPDB resource is allowed.</p> <pre><code>// The DisruptionProbe service definition.\nservice DisruptionProbeService {\n  // Sends a IsDisruptionAllowed request which will check if a given Pod\n  // can be disrupted according to some specific rules.\n  rpc IsDisruptionAllowed(IsDisruptionAllowedRequest) returns (IsDisruptionAllowedResponse) {}\n}\n\n// IsDisruptionAllowedRequest has the information to request a check for disruption.\nmessage IsDisruptionAllowedRequest {\n  // The name of the pod that is being disrupted.\n  string pod_name = 1;\n\n  // The namespaces of the pod that is being disrupted.\n  string pod_namespace = 2;\n\n  // The name of the XPodDisruptionBudget resource that was protecting the pod.\n  string xpdb_name = 3;\n\n  // The namespace of the XPodDisruptionBudget resource that was protecting the pod.\n  string xpdb_namespace = 4;\n}\n\n// IsDisruptionAllowedRespobse has the information on wether a disruption is allowed or not.\nmessage IsDisruptionAllowedResponse {\n  // Information on wether disruption is allowed.\n  bool is_allowed = 1;\n\n  // Error information on why a disruption is not allowed.\n  string error = 2;\n}\n</code></pre> <p>You can check for a sample implementation on <code>cmd/testdisruptionprobe/main.go</code>.</p>"},{"location":"configuring-xpdb/","title":"Configuring XPodDisruption Resource","text":""},{"location":"configuring-xpdb/#nomenclature","title":"Nomenclature","text":"term description local cluster This is the Kubernetes cluster where a pod deletion or eviction is being requested. remote clusters These are the clusters which are not handling the pod eviction / deletion."},{"location":"configuring-xpdb/#configuration-and-behavior","title":"Configuration and Behavior","text":"<p>X-PDB works under the assumption that your workloads are structured in a similar way across clusters, i.e. that pods that you want to protect sit in the same namespace no matter which cluster you look at.</p> <p>The <code>XPodDisruptionBudget</code> resources looks and feels just like <code>PodDisruptionBudget</code> resources:</p> <ul> <li> <p>A label selector <code>.spec.selector</code> to specify the set of pods to which it applies. The <code>selector</code> applies to all clusters for decision making, not just the local one where pod eviction/deletion is happening.</p> </li> <li> <p><code>.spec.minAvailable</code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage.</p> </li> <li><code>.spec.maxUnavailable</code> which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.</li> </ul> <p>You can specify only one of <code>maxUnavailable</code> and <code>minAvailable</code> in a single PodDisruptionBudget. <code>maxUnavailable</code> can only be used to control the eviction of pods that have an associated controller managing them.</p> <p>In addition to that, <code>XPodDisruptionBudget</code> has the following fields:</p> <ul> <li><code>.spec.suspend</code> which allows you to disable the XPDB resource. This allows all pod deletions/evictions. It is intended to be used as a break-glass procedure to allow engineers to take manual action. The suspension is configured on a per-cluster basis and affects only local pods. I.e. other clusters that run x-pdb will not be able to evict pods if there isn't enough disruption budget available globally.</li> <li><code>.spec.probe</code> that allows workload owners to define a disruption probe endpoint. Without a probe, x-pdb will only consider pod readiness as an indicator of healthiness and compute the disruption verdict based on that. With <code>.spec.probe</code>, x-pdb considers the response of the probe endpoint as well.</li> </ul> <p>It is irrelevant for <code>x-pdb</code> if the remote cluster has a <code>XPodDisruptionBudget</code> resource and whether or not the configuration match.</p> <p>The user is supposed to deploy the <code>XPodDisruptionBudget</code> to all clusters. It may lead to unexpected disruptions when the resource is missing.</p> <pre><code>apiVersion: x-pdb.form3.tech/v1alpha1\nkind: XPodDisruptionBudget\nmetadata:\n  name: kube-dns\n  namespace: kube-system\nspec:\n  # Specify either `minAvailable` or `maxUnavailable`\n  # Both percentages and numbers are supported\n  minAvailable: 80%\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n</code></pre>"},{"location":"configuring-xpdb/#grpc-state-server","title":"gRPC State Server","text":"<p>In order for x-pdb servers to communicate between each other they expose a gRPC state server interface with the following APIs. It allows x-pdb to asses the health of pods on remote clusters.</p> <p>The communication between x-pdb servers is secured using mutual TLS. The certificate directory can be configured with <code>--controller-certs-dir</code> which is supposed to contain <code>ca.crt</code>, <code>tls.crt</code> and <code>tls.key</code> files.</p> <pre><code>// State is the service that allows x-pdb servers to talk with\n// each other.\nservice State {\n  // Acquires a lock on the local cluster using the specified leaseHolderIdentity.\n  rpc Lock(LockRequest) returns (LockResponse) {}\n\n  // Frees a lock on the local cluster if the lease identity matches.\n  rpc Unlock(UnlockRequest) returns (UnlockResponse) {}\n\n  // Calculates the expected count based off the Deployment/StatefulSet/ReplicaSet number of replicas or - if implemented - a `scale` sub resource.\n  rpc GetState(GetStateRequest) returns (GetStateResponse) {}\n}\n</code></pre>"},{"location":"developer-guide/","title":"Developer Guide","text":""},{"location":"developer-guide/#getting-started","title":"Getting Started","text":"<p>You must have a working Go environment and then clone the repo:</p> <pre><code>git clone https://github.com/form3tech-oss/x-pdb.git\ncd x-pdb\n</code></pre> <p>You'll need the following tools to work with the project:</p> <ul> <li>Kubectl</li> <li>Kind</li> <li>Helm</li> <li>Kubebuilder</li> <li>Buf</li> </ul>"},{"location":"developer-guide/#build-and-test","title":"Build and Test","text":"<p>The project uses the <code>make</code> build system. It'll run code generators, tests and static code analysis.</p>"},{"location":"developer-guide/#development-environment","title":"Development environment","text":"<p>The development environment is based of 3 kind clusters that are connected together through some loadbalancers using MetalLB.</p> <p>X-PDB on each cluster will expose a LoadBalancer service which the IP is going to be provisioned by MetalLB. That IP is going to be available to all the clusters, allowing X-PDB to talk with other X-PDB services in other clusters.</p> <p>To spin up the development environment you should run the following command:</p> <pre><code>make multi-cluster\n</code></pre> <p>You should run the following command to install x-pdb on all the 3 kind clusters:</p> <pre><code>make deploy\n</code></pre> <p>After it is installed you are able to create workloads and x-pdb resources to test out the features.</p>"},{"location":"developer-guide/#testing","title":"Testing","text":""},{"location":"developer-guide/#unit-tests","title":"Unit tests","text":"<p>X-PDB unit tests can be run with the following command:</p> <pre><code>make test\n</code></pre>"},{"location":"developer-guide/#e2e-tests","title":"E2E Tests","text":"<p>E2E test suite will need the development environment to be up and running. You will need to build and deploy some testing applications by running the following command.</p> <pre><code>make deploy-e2e\n</code></pre> <p>After the testing applications are installed you can run the E2E test suite by running the following command:</p> <pre><code>make e2e\n</code></pre>"},{"location":"developer-guide/#linting","title":"Linting","text":"<p>Before commiting your changes ensure that codebase is linted.</p> <pre><code>make fmt\nmake proto-format\nmake lint\nmake helm-lint\nmake proto-lint\n</code></pre>"},{"location":"developer-guide/#working-with-gprc","title":"Working with gPRC","text":"<p>The gPRC protobuf contracts are declared in:</p> <ul> <li><code>proto/disruptionprobe/v1/disruptionprobe.proto</code></li> <li><code>proto/state/v1/state.proto</code></li> </ul> <p>These contracts are managed by buf. It will allow us to easily manage everything related with gRPC.</p> <p>After you make changes to proto contracts please run the following commands to ensure all changes valid:</p> <pre><code>make proto-format\nmake proto-lint\nmake proto-breaking\nmake proto-generate\n</code></pre>"},{"location":"developer-guide/#documentation","title":"Documentation","text":"<p>Documentation for this project is done by mkdocs.</p> <p>To test out the changes while you're changing documentation run the following command:</p> <pre><code>cd docs\nmake live-docs\n</code></pre>"},{"location":"failure-scenarios/","title":"Failure Scenarios","text":""},{"location":"failure-scenarios/#1-a-remote-cluster-is-unavailable","title":"1. A remote Cluster is unavailable","text":"<p>Scenario: one of the remote clusters is unavailable, e.g. due to a network issue or that all x-pdb pods on the remote cluster are unavailable.</p>"},{"location":"failure-scenarios/#impact-pod-evictionsdeletions-are-blocked","title":"Impact: pod evictions/deletions are blocked","text":"<p>Because the state of the remote clusters is undefined, x-pdb will reject all eviction/deletions. Though this applies only for pods which have a <code>XPodDisruptionBudget</code> configured.</p> <p>\u26a0\ufe0f Warning: This includes blocking node pressure evictions.</p>"},{"location":"failure-scenarios/#mitigation","title":"Mitigation","text":"<p>If a cluster is taken down for maintenance for a extended period of time you should consider removing that cluster from the <code>remoteEndpoint</code> configuration, so that x-pdb ignores that cluster entirely.</p> <p>In an incident scenario you can consider to set the <code>failurePolicy=Ignore</code> to NOT block deletions/evictions. Only do it if you can accept the risk. Your workloads are no longer protected.</p> <p>Further, you can temporarily delete the <code>XPodDisruptionBudget</code> resource or setting <code>spec.suspend=true</code> which effectively make x-pdb ignore it.</p>"},{"location":"failure-scenarios/#2-x-pdb-pods-are-unavailable-within-the-cluster","title":"2. x-pdb pods are unavailable within the cluster","text":"<p>Scenario: x-pdb is unavailable within the cluster, from the perspective of kube-apiserver. That could be due to network policies, TLS misconfiguration, service selector misconfiguration, pods being not ready or other means. It is likely, that x-pdb is also unavailable from the perspective of other clusters (see scenario above).</p>"},{"location":"failure-scenarios/#impact-all-pod-evictionsdeletions-are-blocked","title":"Impact: all pod evictions/deletions are blocked","text":"<p>Depending on the <code>failurePolicy</code> on the webhook, kube-apiserver will reject all pod evictions / deletions due to the unavailability of the webhook.</p> <p>\u26a0\ufe0f Warning: This includes blocking node pressure evictions.</p>"},{"location":"failure-scenarios/#mitigation_1","title":"Mitigation","text":"<p>In an incident scenario you can consider to set the <code>failurePolicy=Ignore</code> to NOT block deletions/evictions. Only do it if you can accept the risk. Your workloads are no longer protected.</p>"},{"location":"failure-scenarios/#reliability-recommendations","title":"Reliability recommendations","text":""},{"location":"failure-scenarios/#1-enable-x-pdb-only-for-specific-workloads","title":"1. Enable x-pdb only for specific workloads","text":"<p>To limit the impact on a remote cluster being unavailable, configure the x-pdb webhook to only apply to certain namespaces.</p> <pre><code>webhook:\n  namespaceSelector: {}\n    matchExpressions:\n      - key: x-pdb.form3.tech/enabled\n        operator: Exists\n</code></pre>"},{"location":"failure-scenarios/#2-use-suspend","title":"2. use <code>suspend</code>","text":"<p>Use XPodDisruptionBudget <code>.spec.suspend</code> to temporarily disable the XPDB in case of an incident. Be aware that your workloads are no longer protected.</p>"},{"location":"failure-scenarios/#3-use-disruption-probes","title":"3. Use disruption probes","text":"<p>Use disruption probes to assess if a workload is stable enough to deal with a disruption.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>x-pdb provides a helm chart for as method of installation. It is recommended to deploy <code>x-pdb</code> into a dedicated namespace and not co-locate it with other applications in existing namespaces such as <code>kube-system</code>.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>x-pdb is supposed to be deployed into multiple clusters. Each x-pdb deployment needs to be accessible from other clusters. Implementation depends on your environment, most likely you'll have a Service with type=LoadBalancer to expose x-pdb to your other clusters.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Helm</li> <li>Have multiple Kubernetes clusters with a supported version</li> <li>Have cert-manager installed or hand cranked TLS certificates ready to be deployed</li> </ul>"},{"location":"getting-started/#1-installing-x-pdb","title":"1. Installing x-pdb","text":"<pre><code>helm repo add x-pdb https://form3tech-oss.github.io/x-pdb/\nhelm repo update\nhelm install x-pdb/x-pdb -n x-pdb \\\n    --create-namespace \\\n    --set controller.clusterID cluster-1\n</code></pre>"},{"location":"getting-started/#2-configuring-x-pdb","title":"2. Configuring x-pdb","text":"<p>A full list of available Helm values is in the x-pdb GitHub repository.</p> <p>The example below shows the most important configuration values which you'll need to set in order to get x-pdb up and running.</p> <pre><code>controller:\n  # set cluster id of _this_ deployment\n  clusterID: \"blue\"\n  # remote endpoint which x-pdb will ask for approving an eviction\n  remoteEndpoints:\n    - x-pdb.lb.grey.cluster.local\n    - x-pdb.lb.gold.cluster.local\n  tls:\n    # let cert-manager mint a certificate for x-pdb controller\n    certManager:\n      enabled: true\n      issuerRef:\n        group: cert-manager.io\n        kind: ClusterIssuer\n        name: my-example-issuer\n      dnsNames:\n        - x-pdb.lb.blue.cluster.local\nservice:\n  controller:\n    type: LoadBalancer\n\nwebhook:\n  # intercept evictions only in certain namespaces\n  namespaceSelector:\n    matchLabels:\n      \"x-pdb-enabled\": \"true\"\n  # let cert-manager mint a certificate for x-pdb webhook\n  tls:\n    certManager:\n      enabled: true\n      issuerRef:\n        group: cert-manager.io\n        kind: ClusterIssuer\n        name: my-example-issuer\npriorityClassName: system-cluster-critical\nserviceMonitor:\n  enabled: true\n</code></pre>"},{"location":"metrics-slos/","title":"Metrics &amp; SLOs","text":"<p>x-pdb exposes Prometheus metrics on the <code>/metrics</code> path. To enable it, set the <code>serviceMonitor.enabled</code> Helm flag to true.</p>"},{"location":"metrics-slos/#metrics","title":"Metrics","text":""},{"location":"metrics-slos/#x-pdb","title":"x-pdb","text":"Name Type Description <code>pod_eviction_rejected</code> Counter Represents the number of eviction which have been rejected through x-pdb. <code>pod_matches_multiple_xpdbs</code> Counter A eviction attempt for a pod has been observed which matches multiple XPDBs. This is a invalid configuration and must be fixed. <code>lock_errors</code> Counter Counter that represents the number of errors when obtaining locks for xpdb."},{"location":"metrics-slos/#grpc-metrics","title":"grpc metrics","text":"<p>x-pdb exposes GRPC metrics for both client and server which allow you to get insights into latency and availability of the remote x-pdb servers.</p>"},{"location":"metrics-slos/#slos","title":"SLOs","text":""},{"location":"metrics-slos/#availability","title":"Availability","text":"<p>There should be at least one pod ready to serve traffic at any time, preferably measured from both <code>kube-apiserver</code> and <code>x-pdb</code> on other clusters.</p> <pre><code>sum(increase(apiserver_admission_webhook_fail_open_count{name=~\".*x-pdb.*\"}[5m]))\n</code></pre>"},{"location":"metrics-slos/#latency","title":"Latency","text":"<p>The amount of time x-pdb needs to respond to a admission webhook, preferably measured from the kube-apiserver. It should take less than 150ms for <code>x-pdb</code> to respond to admission requests on the p99. The threshold may vary in your environment, depending on the cross-cluster latency.</p> <pre><code>histogram_quantile(0.99,\n    sum(rate(apiserver_admission_webhook_admission_duration_seconds_bucket{name=~\".*x-pdb.*\"}[5m])) by (le, name)\n)\n</code></pre>"},{"location":"release/","title":"Release Process","text":"<p>X-PDB release process is performed by the <code>Create Release</code> GHA Workflow.</p> <p>The workflow needs the version of the release, from which branch it will be generated and if it will be a pre-release.</p> <p>This process will perform the following actions:</p> <ol> <li>Build and Publish X-PDB Docker images into GHCR.</li> <li>Package and Publish the HelmChart into GHCR and Github Pages.</li> <li>Create a Github Release with the changelog since the last release.</li> </ol>"}]}